{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰ªªÂä° 5ÔºöÂ§ßÊ®°ÂûãËÉΩÂäõËØÑ‰º∞Ôºà40ÂàÜÔºâ\n",
    "\n",
    "### ‰Ωú‰∏öË¶ÅÊ±Ç\n",
    "\n",
    "‰ªéÂºÄÊ∫êÂíåÈó≠Ê∫ê‰∏§Á±ªÂ§ßÊ®°Âûã‰∏≠ÔºåÊØè‰∏ÄÁ±ªËá≥Â∞ëÈÄâÊã© 2 ‰∏™Ê®°Âûã„ÄÅÊØè‰∏™Ê®°ÂûãËá≥Â∞ëÈÄâÊã© 2 ‰∏™ÁâàÊú¨ÔºåÂèÇËÄÉÊñáÁåÆ‰∏≠ÁªôÂá∫ÁöÑËØÑ‰ª∑ÊåáÊ†áÔºåËØÑ‰º∞‰∏çÂêåÁöÑÂ§ßÊ®°ÂûãÊ±ÇËß£ÁÆÄÂçïÊï∞Â≠¶ÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇËøõ‰∏ÄÊ≠•ÔºåÈÄöËøáÂ∞ùËØï‰ºòÂåñÊèêÁ§∫„ÄÅÊîπËøõÊ®°Âûã CoT Êé®ÁêÜËøáÁ®ãÁ≠âÊñπÊ≥ïÔºåÊèêÂçáÂ§ßÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ\n",
    "\n",
    "### ÂèÇËÄÉÊï∞ÊçÆ\n",
    "\n",
    "[GSM8K](https://huggingface.co/datasets/openai/gsm8k)\n",
    "\n",
    "### Ë¶ÅÊ±Ç\n",
    "\n",
    "Êí∞ÂÜô 5 È°µ‰ª•ÂÜÖÁöÑËØÑÊµãÊä•ÂëäÔºåËá≥Â∞ëÂåÖÂê´‰ΩøÁî®ÁöÑÊ®°ÂûãÂèäÂÖ∂ÁâπÁÇπ„ÄÅ‰ºòÂåñÂêéÁöÑ Prompt„ÄÅÊ®°ÂûãÊÄßËÉΩÂØπÊØî‰∏éÂàÜÊûêÁ≠â„ÄÇ\n",
    "\n",
    "### ÂèÇËÄÉÊñáÁåÆ\n",
    " - [Karl Cobbe, et, at. Training Verifiers to Solve Math Word Problems. 2021.](https://arxiv.org/abs/2110.14168) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Ê∫êÁ†Å üëá\n",
    " - ÂºÄÊ∫êÊ®°Âûã\n",
    "   - GPT-NeoX-20B\n",
    "   - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)\n",
    "     - ChatGLM2\n",
    "     - ChatGLM4-9B\n",
    " - Èó≠Ê∫êÊ®°Âûã\n",
    "   - GPT\n",
    "    - GPT-4\n",
    "    - GPT-4o\n",
    "   - B\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init logger\n",
    "import logging, sys, codecs\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.FileHandler('evaluate.log', encoding='utf-8', mode='w')\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_test_path = \"../data/gsm8k/test-00000-of-00001.parquet\"\n",
    "gsm8k_train_path = \"../data/gsm8k/train-00000-of-00001.parquet\"\n",
    "\n",
    "ds_test = Dataset.from_parquet(gsm8k_test_path)\n",
    "ds_train = Dataset.from_parquet(gsm8k_train_path)\n",
    "\n",
    "print(f\"Train Dataset Sample: {ds_train[0]}\")\n",
    "print(f\"Test Dataset Sample: {ds_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÂºÄÊ∫êÊ®°Âûã 1.1ÔºöLLAMA-1-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Âä†ËΩΩÊ®°ÂûãÂíå tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "def evaluate_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import openai\n",
    "\n",
    "# GPT-4 APIÈÖçÁΩÆ\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# Âä†ËΩΩÂºÄÊ∫êÊ®°ÂûãÔºöGPT-NeoX\n",
    "def load_gpt_neox():\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ‰ΩøÁî®ÂºÄÊ∫êÊ®°ÂûãËøõË°åÊé®ÁêÜ\n",
    "def infer_with_gpt_neox(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ‰ΩøÁî®Èó≠Ê∫êÊ®°ÂûãÔºöGPT-4\n",
    "def infer_with_gpt4(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# Á§∫‰æãÔºöÂä†ËΩΩÂπ∂ÊµãËØïÊ®°Âûã\n",
    "model, tokenizer = load_gpt_neox()\n",
    "sample_prompt = \"Let's solve 25 + 32 step by step.\"\n",
    "gpt_neox_result = infer_with_gpt_neox(model, tokenizer, sample_prompt)\n",
    "gpt4_result = infer_with_gpt4(sample_prompt)\n",
    "\n",
    "print(\"GPT-NeoX Result:\", gpt_neox_result)\n",
    "print(\"GPT-4 Result:\", gpt4_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Êé®ÁêÜÊèêÁ§∫ÁîüÊàêÂáΩÊï∞\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this step by step: {problem}\"\n",
    "\n",
    "# Á§∫‰æãÈóÆÈ¢ò\n",
    "problem = \"What is 25 + 32?\"\n",
    "cot_prompt = generate_cot_prompt(problem)\n",
    "\n",
    "# ÊµãËØï CoT ÊèêÁ§∫ÁöÑÊé®ÁêÜÁªìÊûú\n",
    "gpt_neox_cot_result = infer_with_gpt_neox(model, tokenizer, cot_prompt)\n",
    "gpt4_cot_result = infer_with_gpt4(cot_prompt)\n",
    "\n",
    "print(\"GPT-NeoX CoT Result:\", gpt_neox_cot_result)\n",
    "print(\"GPT-4 CoT Result:\", gpt4_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, tokenizer, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        problem = example[\"problem\"]\n",
    "        correct_answer = example[\"answer\"]\n",
    "        \n",
    "        # Ê†πÊçÆÊòØÂê¶ÂêØÁî®CoTÊèêÁ§∫ÔºåÈÄâÊã©‰∏çÂêåÁöÑÊèêÁ§∫ÊñπÂºè\n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(problem)\n",
    "        else:\n",
    "            prompt = problem\n",
    "        \n",
    "        # Ëé∑ÂèñÊ®°ÂûãÁöÑÁ≠îÊ°à\n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        else:\n",
    "            model_result = infer_with_gpt_neox(model, tokenizer, prompt)\n",
    "        \n",
    "        # ÁÆÄÂçïÁöÑÂáÜÁ°ÆÊÄßÂà§Êñ≠ÔºåÂøΩÁï•Á©∫Ê†ºÂíåÊ†áÁÇπ\n",
    "        if correct_answer.strip() == model_result.strip():\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "# ËØÑ‰º∞GPT-4ÂíåGPT-NeoXÁöÑÂáÜÁ°ÆÊÄß\n",
    "gpt_neox_accuracy = evaluate_accuracy(model, tokenizer, train_data, cot_prompt=True)\n",
    "gpt4_accuracy = evaluate_accuracy(\"gpt4\", None, train_data, cot_prompt=True)\n",
    "\n",
    "print(f\"GPT-NeoX Accuracy: {gpt_neox_accuracy * 100:.2f}%\")\n",
    "print(f\"GPT-4 Accuracy: {gpt4_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÂºÄÊ∫êÊ®°Âûã 2.1 ChatGLM2-6B\n",
    "\n",
    "### Ê®°ÂûãÂÆûÁé∞ÂíåÊùÉÈáç\n",
    "\n",
    "Áî±‰∫éÊú¨Âú∞ËøûÊé• Huggingface ÁöÑÁΩëÁªúÁéØÂ¢É‰∏ç‰Ω≥ÔºåÊïÖÊ®°ÂûãÂÆûÁé∞ÂíåÊùÉÈáçÂùá‰∏∫Êú¨Âú∞Âä†ËΩΩ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/chatglm2-6b\"\n",
    "tokenizer_glm2_6b = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model_glm2_6b = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device='cuda')\n",
    "model_glm2_6b = model_glm2_6b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm2_6b(question, history=[]):\n",
    "    response, _history = model_glm2_6b.chat(tokenizer_glm2_6b, question, history=[])\n",
    "    return response, _history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÂºÄÊ∫êÊ®°Âûã 2.2 ChatGLM4-9B\n",
    "\n",
    "- **[ATTENTION]**\n",
    "    - Êé®ÁêÜ `ChatGLM4-9B`ÂâçÔºåÈúÄË¶ÅÂçáÁ∫ß Transformers Âà∞ `>= 4.46.0` ÁâàÊú¨ÔºåÂê¶Âàô‰ºöÂá∫Áé∞ÈîôËØØ„ÄÇ\n",
    "    - `Python` == `3.10.12`\n",
    "\n",
    "ÂèÇËÄÉËµÑÊñôÔºö\n",
    " - [[FIXED] Exception: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3](https://github.com/unslothai/unsloth/issues/1059)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/glm-4-9b-chat-hf\"\n",
    "\n",
    "tokenizer_glm4_9b_hf = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_PATH, trust_remote_code=True)\n",
    "model_glm4_9b_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm4_9b_hf(question):\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer the following question. At the end of you answer, include 'The answer: xxx.', xxx is a number. \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    inputs = tokenizer_glm4_9b_hf.apply_chat_template(message,\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )\n",
    "    try:    \n",
    "        inputs['input_ids'] = inputs['input_ids'].to('cuda')\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to('cuda')\n",
    "    except:\n",
    "        pass\n",
    "           \n",
    "    gen_kwargs = {\"max_length\": 4000, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_glm4_9b_hf.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        print(tokenizer_glm4_9b_hf.decode(outputs[0], skip_special_tokens=True))\n",
    "        return tokenizer_glm4_9b_hf.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Èó≠Ê∫êÊ®°Âûã 1.1 GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Èó≠Ê∫êÊ®°Âûã 1.2 GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Èó≠Ê∫êÊ®°Âûã 2.1 openai-o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Èó≠Ê∫êÊ®°Âûã 2.2 openai-o1-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM ËØÑ‰ª∑\n",
    "\n",
    "ËøôÈÉ®ÂàÜÂ∞ÜÂü∫‰∫é `GSM8K` Math Solving Êï∞ÊçÆÈõÜÔºåÂØπ‰∏äËø∞ÁöÑ 8 ‰∏™Ê®°ÂûãËøõË°åËØÑ‰ª∑„ÄÇ\n",
    "\n",
    "ËøôÈáåÊ†πÊçÆÂèÇËÄÉÊñáÁåÆ‰∏≠ÁöÑ `CoT` Êé®ÁêÜÊñπÊ≥ïËøõË°åÂÆûÈ™åÔºåËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÂú® 1396 ‰∏™ÊµãËØïÊ†∑Êú¨‰∏äÁöÑÁªìÊûúÊ≠£Á°ÆÊÄß„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(truth:str, answer:str) -> bool:\n",
    "    \"\"\"_summary_\n",
    "        extract the numbers from the answer.\n",
    "        \n",
    "        e.g. Truth: \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. \\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer‚Äôs market.\\n#### 18\"\n",
    "        \n",
    "        e.g. Answer: \"First find how many eggs Janet eats each day: 16 eggs / day - 3 eggs / day = 13 eggs / day. Then find how many eggs she bakes each day: 13 eggs / day * 4 eggs / day = 52 eggs / day. Then find the total number of eggs she sells each day: 52 eggs / day - 13 eggs / day = 39 eggs / day. Then multiply the number of eggs she sells by the price per egg to find her total earnings: 39 eggs / day * $2 / egg = $78 / day.\\nThe answer: 78.\"\n",
    "        \n",
    "        In Truth, the number '18' after the last '####' is the final answer.\n",
    "        In Answer, the number '78' after the last 'The answer: ' is the final answer.\n",
    "    \"\"\"\n",
    "    logging.info(f\"[Judge] [Truth]: {truth}\")\n",
    "    logging.info(f\"[Judge] [Answer]: {answer}\")\n",
    "    success = False\n",
    "    try:\n",
    "        truth_num = int(truth.split('####')[-1].strip())\n",
    "        answer_num = answer.split('The answer: ')[-1].strip()\n",
    "        if answer_num.endswith('.'):\n",
    "            answer_num = answer_num.split('.')[0].strip()\n",
    "        answer_num = int(answer_num)\n",
    "        success = (truth_num == answer_num)\n",
    "        logging.info(f\"[Judge] {success}. Truth: {truth_num}, Answer: {answer_num}\")\n",
    "    except Exception as e:\n",
    "        success = False\n",
    "        logging.info(f\"[Judge] Skip this question for patten dismatch: {e}\")\n",
    "    finally:\n",
    "        return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Êé®ÁêÜÊèêÁ§∫ÁîüÊàêÂáΩÊï∞\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this problem step by step: {problem}\"\n",
    "\n",
    "# ËØÑ‰ª∑ÂáΩÊï∞\n",
    "def evaluate_accuracy(model, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    current_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        question = example[\"question\"]\n",
    "        truth = example[\"answer\"]\n",
    "        \n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(question)\n",
    "        else:\n",
    "            prompt = question\n",
    "        \n",
    "        current_count += 1\n",
    "        logging.info(f\"[Evaluate] eval at {current_count}/{total_count}\")\n",
    "        \n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        elif isinstance(model, str) and model == \"glm2-6b\":\n",
    "            model_result, _ = infer_with_glm2_6b(prompt)\n",
    "        elif isinstance(model, str) and model == \"glm4_9b_hf\":\n",
    "            model_result = infer_with_glm4_9b_hf(prompt)\n",
    "        else:\n",
    "            print(f\"Model '{model}' is not supported.\")\n",
    "            return\n",
    "        \n",
    "        if judge(truth, model_result):\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm2_6b_accuracy = evaluate_accuracy(\"glm2-6b\", dataset=ds_test, cot_prompt=True)\n",
    "glm4_9b_hf_accuracy = evaluate_accuracy(\"glm4_9b_hf\", dataset=ds_test, cot_prompt=True)\n",
    "\n",
    "# print(f\"ChatGLM2-6B Accuracy: {glm2_6b_accuracy * 100:.2f}%\")\n",
    "print(f\"GLM4-9B-HF Accuracy: {glm4_9b_hf_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
