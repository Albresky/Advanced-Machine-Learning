{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务 5：大模型能力评估（40分）\n",
    "\n",
    "### 作业要求\n",
    "\n",
    "从开源和闭源两类大模型中，每一类至少选择 2 个模型、每个模型至少选择 2 个版本，参考文献中给出的评价指标，评估不同的大模型求解简单数学问题的能力。进一步，通过尝试优化提示、改进模型 CoT 推理过程等方法，提升大模型的性能。\n",
    "\n",
    "### 参考数据\n",
    "\n",
    "[GSM8K](https://huggingface.co/datasets/openai/gsm8k)\n",
    "\n",
    "### 要求\n",
    "\n",
    "撰写 5 页以内的评测报告，至少包含使用的模型及其特点、优化后的 Prompt、模型性能对比与分析等。\n",
    "\n",
    "### 参考文献\n",
    " - [Karl Cobbe, et, at. Training Verifiers to Solve Math Word Problems. 2021.](https://arxiv.org/abs/2110.14168) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 源码 👇\n",
    " - 开源模型\n",
    "   - GPT-NeoX-20B\n",
    "   - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)\n",
    "     - ChatGLM2\n",
    "     - ChatGLM4-9B\n",
    " - 闭源模型\n",
    "   - GPT\n",
    "    - GPT-4\n",
    "    - GPT-4o\n",
    "   - B\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init logger\n",
    "import logging, sys, codecs\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.FileHandler('evaluate.log', encoding='utf-8', mode='w')\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_test_path = \"../data/gsm8k/test-00000-of-00001.parquet\"\n",
    "gsm8k_train_path = \"../data/gsm8k/train-00000-of-00001.parquet\"\n",
    "\n",
    "ds_test = Dataset.from_parquet(gsm8k_test_path)\n",
    "ds_train = Dataset.from_parquet(gsm8k_train_path)\n",
    "\n",
    "print(f\"Train Dataset Sample: {ds_train[0]}\")\n",
    "print(f\"Test Dataset Sample: {ds_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 开源模型 1.1：LLAMA-1-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "def evaluate_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import openai\n",
    "\n",
    "# GPT-4 API配置\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# 加载开源模型：GPT-NeoX\n",
    "def load_gpt_neox():\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 使用开源模型进行推理\n",
    "def infer_with_gpt_neox(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 使用闭源模型：GPT-4\n",
    "def infer_with_gpt4(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# 示例：加载并测试模型\n",
    "model, tokenizer = load_gpt_neox()\n",
    "sample_prompt = \"Let's solve 25 + 32 step by step.\"\n",
    "gpt_neox_result = infer_with_gpt_neox(model, tokenizer, sample_prompt)\n",
    "gpt4_result = infer_with_gpt4(sample_prompt)\n",
    "\n",
    "print(\"GPT-NeoX Result:\", gpt_neox_result)\n",
    "print(\"GPT-4 Result:\", gpt4_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT 推理提示生成函数\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this step by step: {problem}\"\n",
    "\n",
    "# 示例问题\n",
    "problem = \"What is 25 + 32?\"\n",
    "cot_prompt = generate_cot_prompt(problem)\n",
    "\n",
    "# 测试 CoT 提示的推理结果\n",
    "gpt_neox_cot_result = infer_with_gpt_neox(model, tokenizer, cot_prompt)\n",
    "gpt4_cot_result = infer_with_gpt4(cot_prompt)\n",
    "\n",
    "print(\"GPT-NeoX CoT Result:\", gpt_neox_cot_result)\n",
    "print(\"GPT-4 CoT Result:\", gpt4_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, tokenizer, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        problem = example[\"problem\"]\n",
    "        correct_answer = example[\"answer\"]\n",
    "        \n",
    "        # 根据是否启用CoT提示，选择不同的提示方式\n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(problem)\n",
    "        else:\n",
    "            prompt = problem\n",
    "        \n",
    "        # 获取模型的答案\n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        else:\n",
    "            model_result = infer_with_gpt_neox(model, tokenizer, prompt)\n",
    "        \n",
    "        # 简单的准确性判断，忽略空格和标点\n",
    "        if correct_answer.strip() == model_result.strip():\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "# 评估GPT-4和GPT-NeoX的准确性\n",
    "gpt_neox_accuracy = evaluate_accuracy(model, tokenizer, train_data, cot_prompt=True)\n",
    "gpt4_accuracy = evaluate_accuracy(\"gpt4\", None, train_data, cot_prompt=True)\n",
    "\n",
    "print(f\"GPT-NeoX Accuracy: {gpt_neox_accuracy * 100:.2f}%\")\n",
    "print(f\"GPT-4 Accuracy: {gpt4_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 开源模型 2.1 ChatGLM2-6B\n",
    "\n",
    "### 模型实现和权重\n",
    "\n",
    "由于本地连接 Huggingface 的网络环境不佳，故模型实现和权重均为本地加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/chatglm2-6b\"\n",
    "tokenizer_glm2_6b = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model_glm2_6b = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device='cuda')\n",
    "model_glm2_6b = model_glm2_6b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm2_6b(question, history=[]):\n",
    "    response, _history = model_glm2_6b.chat(tokenizer_glm2_6b, question, history=[])\n",
    "    return response, _history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 开源模型 2.2 ChatGLM4-9B\n",
    "\n",
    "- **[ATTENTION]**\n",
    "    - 推理 `ChatGLM4-9B`前，需要升级 Transformers 到 `>= 4.46.0` 版本，否则会出现错误。\n",
    "    - `Python` == `3.10.12`\n",
    "\n",
    "参考资料：\n",
    " - [[FIXED] Exception: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3](https://github.com/unslothai/unsloth/issues/1059)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/glm-4-9b-chat-hf\"\n",
    "\n",
    "tokenizer_glm4_9b_hf = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_PATH, trust_remote_code=True)\n",
    "model_glm4_9b_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm4_9b_hf(question):\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer the following question. At the end of you answer, include 'The answer: xxx.', xxx is a number. \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    inputs = tokenizer_glm4_9b_hf.apply_chat_template(message,\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )\n",
    "    try:    \n",
    "        inputs['input_ids'] = inputs['input_ids'].to('cuda')\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to('cuda')\n",
    "    except:\n",
    "        pass\n",
    "           \n",
    "    gen_kwargs = {\"max_length\": 4000, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_glm4_9b_hf.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        print(tokenizer_glm4_9b_hf.decode(outputs[0], skip_special_tokens=True))\n",
    "        return tokenizer_glm4_9b_hf.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 闭源模型 1.1 GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 闭源模型 1.2 GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 闭源模型 2.1 openai-o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 闭源模型 2.2 openai-o1-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM 评价\n",
    "\n",
    "这部分将基于 `GSM8K` Math Solving 数据集，对上述的 8 个模型进行评价。\n",
    "\n",
    "这里根据参考文献中的 `CoT` 推理方法进行实验，评估这些模型在 1396 个测试样本上的结果正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(truth:str, answer:str) -> bool:\n",
    "    \"\"\"_summary_\n",
    "        extract the numbers from the answer.\n",
    "        \n",
    "        e.g. Truth: \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. \\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18\"\n",
    "        \n",
    "        e.g. Answer: \"First find how many eggs Janet eats each day: 16 eggs / day - 3 eggs / day = 13 eggs / day. Then find how many eggs she bakes each day: 13 eggs / day * 4 eggs / day = 52 eggs / day. Then find the total number of eggs she sells each day: 52 eggs / day - 13 eggs / day = 39 eggs / day. Then multiply the number of eggs she sells by the price per egg to find her total earnings: 39 eggs / day * $2 / egg = $78 / day.\\nThe answer: 78.\"\n",
    "        \n",
    "        In Truth, the number '18' after the last '####' is the final answer.\n",
    "        In Answer, the number '78' after the last 'The answer: ' is the final answer.\n",
    "    \"\"\"\n",
    "    logging.info(f\"[Judge] [Truth]: {truth}\")\n",
    "    logging.info(f\"[Judge] [Answer]: {answer}\")\n",
    "    success = False\n",
    "    try:\n",
    "        truth_num = int(truth.split('####')[-1].strip())\n",
    "        answer_num = answer.split('The answer: ')[-1].strip()\n",
    "        if answer_num.endswith('.'):\n",
    "            answer_num = answer_num.split('.')[0].strip()\n",
    "        answer_num = int(answer_num)\n",
    "        success = (truth_num == answer_num)\n",
    "        logging.info(f\"[Judge] {success}. Truth: {truth_num}, Answer: {answer_num}\")\n",
    "    except Exception as e:\n",
    "        success = False\n",
    "        logging.info(f\"[Judge] Skip this question for patten dismatch: {e}\")\n",
    "    finally:\n",
    "        return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT 推理提示生成函数\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this problem step by step: {problem}\"\n",
    "\n",
    "# 评价函数\n",
    "def evaluate_accuracy(model, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    current_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        question = example[\"question\"]\n",
    "        truth = example[\"answer\"]\n",
    "        \n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(question)\n",
    "        else:\n",
    "            prompt = question\n",
    "        \n",
    "        current_count += 1\n",
    "        logging.info(f\"[Evaluate] eval at {current_count}/{total_count}\")\n",
    "        \n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        elif isinstance(model, str) and model == \"glm2-6b\":\n",
    "            model_result, _ = infer_with_glm2_6b(prompt)\n",
    "        elif isinstance(model, str) and model == \"glm4_9b_hf\":\n",
    "            model_result = infer_with_glm4_9b_hf(prompt)\n",
    "        else:\n",
    "            print(f\"Model '{model}' is not supported.\")\n",
    "            return\n",
    "        \n",
    "        if judge(truth, model_result):\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm2_6b_accuracy = evaluate_accuracy(\"glm2-6b\", dataset=ds_test, cot_prompt=True)\n",
    "glm4_9b_hf_accuracy = evaluate_accuracy(\"glm4_9b_hf\", dataset=ds_test, cot_prompt=True)\n",
    "\n",
    "# print(f\"ChatGLM2-6B Accuracy: {glm2_6b_accuracy * 100:.2f}%\")\n",
    "print(f\"GLM4-9B-HF Accuracy: {glm4_9b_hf_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
