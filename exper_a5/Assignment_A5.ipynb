{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»»åŠ¡ 5ï¼šå¤§æ¨¡å‹èƒ½åŠ›è¯„ä¼°ï¼ˆ40åˆ†ï¼‰\n",
    "\n",
    "### ä½œä¸šè¦æ±‚\n",
    "\n",
    "ä»å¼€æºå’Œé—­æºä¸¤ç±»å¤§æ¨¡å‹ä¸­ï¼Œæ¯ä¸€ç±»è‡³å°‘é€‰æ‹© 2 ä¸ªæ¨¡å‹ã€æ¯ä¸ªæ¨¡å‹è‡³å°‘é€‰æ‹© 2 ä¸ªç‰ˆæœ¬ï¼Œå‚è€ƒæ–‡çŒ®ä¸­ç»™å‡ºçš„è¯„ä»·æŒ‡æ ‡ï¼Œè¯„ä¼°ä¸åŒçš„å¤§æ¨¡å‹æ±‚è§£ç®€å•æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚è¿›ä¸€æ­¥ï¼Œé€šè¿‡å°è¯•ä¼˜åŒ–æç¤ºã€æ”¹è¿›æ¨¡å‹ CoT æ¨ç†è¿‡ç¨‹ç­‰æ–¹æ³•ï¼Œæå‡å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚\n",
    "\n",
    "### å‚è€ƒæ•°æ®\n",
    "\n",
    "[GSM8K](https://huggingface.co/datasets/openai/gsm8k)\n",
    "\n",
    "### è¦æ±‚\n",
    "\n",
    "æ’°å†™ 5 é¡µä»¥å†…çš„è¯„æµ‹æŠ¥å‘Šï¼Œè‡³å°‘åŒ…å«ä½¿ç”¨çš„æ¨¡å‹åŠå…¶ç‰¹ç‚¹ã€ä¼˜åŒ–åçš„ Promptã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ä¸åˆ†æç­‰ã€‚\n",
    "\n",
    "### å‚è€ƒæ–‡çŒ®\n",
    " - [Karl Cobbe, et, at. Training Verifiers to Solve Math Word Problems. 2021.](https://arxiv.org/abs/2110.14168) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## æºç  ğŸ‘‡\n",
    " - å¼€æºæ¨¡å‹\n",
    "   - GPT-NeoX-20B\n",
    "   - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)\n",
    "     - ChatGLM2\n",
    "     - ChatGLM4-9B\n",
    " - é—­æºæ¨¡å‹\n",
    "   - GPT\n",
    "    - GPT-4\n",
    "    - GPT-4o\n",
    "   - B\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init logger\n",
    "import logging, sys, codecs\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.FileHandler('evaluate.log', encoding='utf-8', mode='w')\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Sample: {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n",
      "Test Dataset Sample: {'question': \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmerâ€™s market.\\n#### 18'}\n"
     ]
    }
   ],
   "source": [
    "gsm8k_test_path = \"../data/gsm8k/test-00000-of-00001.parquet\"\n",
    "gsm8k_train_path = \"../data/gsm8k/train-00000-of-00001.parquet\"\n",
    "\n",
    "ds_test = Dataset.from_parquet(gsm8k_test_path)\n",
    "ds_train = Dataset.from_parquet(gsm8k_train_path)\n",
    "\n",
    "print(f\"Train Dataset Sample: {ds_train[0]}\")\n",
    "print(f\"Test Dataset Sample: {ds_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å¼€æºæ¨¡å‹ 1.1ï¼šLLAMA-1-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œ tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "def evaluate_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPT2Tokenizer\n",
    "import openai\n",
    "\n",
    "# GPT-4 APIé…ç½®\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# åŠ è½½å¼€æºæ¨¡å‹ï¼šGPT-NeoX\n",
    "def load_gpt_neox():\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ä½¿ç”¨å¼€æºæ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "def infer_with_gpt_neox(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ä½¿ç”¨é—­æºæ¨¡å‹ï¼šGPT-4\n",
    "def infer_with_gpt4(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# ç¤ºä¾‹ï¼šåŠ è½½å¹¶æµ‹è¯•æ¨¡å‹\n",
    "model, tokenizer = load_gpt_neox()\n",
    "sample_prompt = \"Let's solve 25 + 32 step by step.\"\n",
    "gpt_neox_result = infer_with_gpt_neox(model, tokenizer, sample_prompt)\n",
    "gpt4_result = infer_with_gpt4(sample_prompt)\n",
    "\n",
    "print(\"GPT-NeoX Result:\", gpt_neox_result)\n",
    "print(\"GPT-4 Result:\", gpt4_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT æ¨ç†æç¤ºç”Ÿæˆå‡½æ•°\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this step by step: {problem}\"\n",
    "\n",
    "# ç¤ºä¾‹é—®é¢˜\n",
    "problem = \"What is 25 + 32?\"\n",
    "cot_prompt = generate_cot_prompt(problem)\n",
    "\n",
    "# æµ‹è¯• CoT æç¤ºçš„æ¨ç†ç»“æœ\n",
    "gpt_neox_cot_result = infer_with_gpt_neox(model, tokenizer, cot_prompt)\n",
    "gpt4_cot_result = infer_with_gpt4(cot_prompt)\n",
    "\n",
    "print(\"GPT-NeoX CoT Result:\", gpt_neox_cot_result)\n",
    "print(\"GPT-4 CoT Result:\", gpt4_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, tokenizer, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        problem = example[\"problem\"]\n",
    "        correct_answer = example[\"answer\"]\n",
    "        \n",
    "        # æ ¹æ®æ˜¯å¦å¯ç”¨CoTæç¤ºï¼Œé€‰æ‹©ä¸åŒçš„æç¤ºæ–¹å¼\n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(problem)\n",
    "        else:\n",
    "            prompt = problem\n",
    "        \n",
    "        # è·å–æ¨¡å‹çš„ç­”æ¡ˆ\n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        else:\n",
    "            model_result = infer_with_gpt_neox(model, tokenizer, prompt)\n",
    "        \n",
    "        # ç®€å•çš„å‡†ç¡®æ€§åˆ¤æ–­ï¼Œå¿½ç•¥ç©ºæ ¼å’Œæ ‡ç‚¹\n",
    "        if correct_answer.strip() == model_result.strip():\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "# è¯„ä¼°GPT-4å’ŒGPT-NeoXçš„å‡†ç¡®æ€§\n",
    "gpt_neox_accuracy = evaluate_accuracy(model, tokenizer, train_data, cot_prompt=True)\n",
    "gpt4_accuracy = evaluate_accuracy(\"gpt4\", None, train_data, cot_prompt=True)\n",
    "\n",
    "print(f\"GPT-NeoX Accuracy: {gpt_neox_accuracy * 100:.2f}%\")\n",
    "print(f\"GPT-4 Accuracy: {gpt4_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å¼€æºæ¨¡å‹ 2.1 ChatGLM2-6B\n",
    "\n",
    "### æ¨¡å‹å®ç°å’Œæƒé‡\n",
    "\n",
    "ç”±äºæœ¬åœ°è¿æ¥ Huggingface çš„ç½‘ç»œç¯å¢ƒä¸ä½³ï¼Œæ•…æ¨¡å‹å®ç°å’Œæƒé‡å‡ä¸ºæœ¬åœ°åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../data/chatglm2-6b\"\n",
    "tokenizer_glm2_6b = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model_glm2_6b = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device='cuda')\n",
    "model_glm2_6b = model_glm2_6b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm2_6b(question, history=[]):\n",
    "    response, _history = model_glm2_6b.chat(tokenizer_glm2_6b, question, history=[])\n",
    "    return response, _history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(truth:str, answer:str) -> bool:\n",
    "    \"\"\"_summary_\n",
    "        extract the numbers from the answer.\n",
    "        \n",
    "        e.g. Truth: \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. \\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmerâ€™s market.\\n#### 18\"\n",
    "        \n",
    "        e.g. Answer: \"First find how many eggs Janet eats each day: 16 eggs / day - 3 eggs / day = 13 eggs / day. Then find how many eggs she bakes each day: 13 eggs / day * 4 eggs / day = 52 eggs / day. Then find the total number of eggs she sells each day: 52 eggs / day - 13 eggs / day = 39 eggs / day. Then multiply the number of eggs she sells by the price per egg to find her total earnings: 39 eggs / day * $2 / egg = $78 / day.\\nThe answer: 78.\"\n",
    "        \n",
    "        In Truth, the number '18' after the last '####' is the final answer.\n",
    "        In Answer, the number '78' after the last 'The answer: ' is the final answer.\n",
    "    \"\"\"\n",
    "    logging.info(f\"[Judge] [Truth]: {truth}\")\n",
    "    logging.info(f\"[Judge] [Answer]: {answer}\")\n",
    "    success = False\n",
    "    try:\n",
    "        truth_num = int(truth.split('####')[-1].strip())\n",
    "        answer_num = int(answer.split('The answer: ')[-1].split('.')[0].strip())\n",
    "        success = (truth_num == answer_num)\n",
    "        logging.info(f\"[Judge] {success}. Truth: {truth_num}, Answer: {answer_num}\")\n",
    "    except Exception as e:\n",
    "        success = False\n",
    "        logging.info(f\"[Judge] Skip this question for patten dismatch: {e}\")\n",
    "    finally:\n",
    "        return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å¼€æºæ¨¡å‹ 2.2 ChatGLM4-9B\n",
    "\n",
    "**[ATTENTION]** æ¨ç† `ChatGLM4-9B`å‰ï¼Œéœ€è¦å‡çº§ Transformers åˆ° `>= 4.46.0` ç‰ˆæœ¬ï¼Œå¦åˆ™ä¼šå‡ºç°é”™è¯¯ã€‚\n",
    "\n",
    "å‚è€ƒèµ„æ–™ï¼š\n",
    " - [[FIXED] Exception: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3](https://github.com/unslothai/unsloth/issues/1059)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/glm-4-9b-chat-hf\"\n",
    "\n",
    "tokenizer_glm4_9b_hf = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_PATH, trust_remote_code=True)\n",
    "model_glm4_9b_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_glm2_9b_hf(question, model, history=[]):\n",
    "    inputs = tokenizer_glm4_9b_hf.apply_chat_template([{\"role\": \"user\", \"content\": question}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )\n",
    "    gen_kwargs = {\"max_length\": 4000, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_glm4_9b_hf.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é—­æºæ¨¡å‹ 1.1 GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é—­æºæ¨¡å‹ 1.2 GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é—­æºæ¨¡å‹ 2.1 openai-o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é—­æºæ¨¡å‹ 2.2 openai-o1-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM è¯„ä»·\n",
    "\n",
    "è¿™éƒ¨åˆ†å°†åŸºäº `GSM8K` Math Solving æ•°æ®é›†ï¼Œå¯¹ä¸Šè¿°çš„ 8 ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä»·ã€‚\n",
    "\n",
    "è¿™é‡Œæ ¹æ®å‚è€ƒæ–‡çŒ®ä¸­çš„ `CoT` æ¨ç†æ–¹æ³•è¿›è¡Œå®éªŒï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ 1396 ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šçš„ç»“æœæ­£ç¡®æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT æ¨ç†æç¤ºç”Ÿæˆå‡½æ•°\n",
    "def generate_cot_prompt(problem):\n",
    "    return f\"Let's solve this problem step by step: {problem}\"\n",
    "\n",
    "# è¯„ä»·å‡½æ•°\n",
    "def evaluate_accuracy(model, dataset, cot_prompt=False):\n",
    "    correct_count = 0\n",
    "    current_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    for example in dataset:\n",
    "        question = example[\"question\"]\n",
    "        truth = example[\"answer\"]\n",
    "        \n",
    "        if cot_prompt:\n",
    "            prompt = generate_cot_prompt(question)\n",
    "        else:\n",
    "            prompt = question\n",
    "        \n",
    "        current_count += 1\n",
    "        logging.info(f\"[Evaluate] eval at {current_count}/{total_count}\")\n",
    "        \n",
    "        if isinstance(model, str) and model == \"gpt4\":\n",
    "            model_result = infer_with_gpt4(prompt)\n",
    "        elif isinstance(model, str) and model == \"glm2-6b\":\n",
    "            model_result, _ = infer_with_glm2_6b(prompt)\n",
    "        elif isinstance(model, tuple) and model[0] == \"glm4_9b_hf\":\n",
    "            model_result = infer_with_glm2_9b_hf(prompt)\n",
    "        else:\n",
    "            print(f\"Model '{model}' is not supported.\")\n",
    "            return\n",
    "        \n",
    "        if judge(truth, model_result):\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm2_6b_accuracy = evaluate_accuracy(\"glm2-6b\", dataset=ds_test, cot_prompt=True)\n",
    "# glm4_9b_hf_accuracy = evaluate_accuracy(\"glm4_9b_hf\", dataset=ds_test, cot_prompt=True)\n",
    "\n",
    "print(f\"ChatGLM2-6B Accuracy: {glm2_6b_accuracy * 100:.2f}%\")\n",
    "# print(f\"GLM4-9B-HF Accuracy: {glm4_9b_hf_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
