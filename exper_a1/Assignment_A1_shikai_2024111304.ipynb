{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Fixed seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "\n",
    "# dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 512)  # First layer, 3*32*32 size\n",
    "        self.fc2 = nn.Linear(512, 256)  # Second layer\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third layer\n",
    "        self.fc4 = nn.Linear(128, 10)  # Fourth layer, 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 32 * 32)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "model = FCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()    # MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()    # L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Adam optimizer with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)  # SGD optimizer with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()  # clear gradients\n",
    "        output = model(data)  # forward pass\n",
    "        loss = criterion(output, target)  # compute loss\n",
    "        loss.backward()  # backward pass\n",
    "        optimizer.step()  # update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1) # fetch the predicted class\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Testing function\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()  # clear\n",
    "        output = model(data)  # forward pass\n",
    "        \n",
    "        # one-hot encoding for MSE\n",
    "        target_one_hot = torch.zeros(data.size(0), 10).to(data.device)\n",
    "        target_one_hot.scatter_(1, target.view(-1, 1), 1)\n",
    "        \n",
    "        loss = criterion(output, target_one_hot)  # calculate loss\n",
    "        loss.backward()  # backward loss\n",
    "        optimizer.step()  # update w\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1) \n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Testing Function\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "\n",
    "            target_one_hot = torch.zeros(data.size(0), 10).to(data.device)\n",
    "            target_one_hot.scatter_(1, target.view(-1, 1), 1)\n",
    "            \n",
    "            loss = criterion(output, target_one_hot)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 0.0962, Train Accuracy: 26.48%\n",
      "Test Loss: 0.0978, Test Accuracy: 22.90%\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.0959, Train Accuracy: 26.81%\n",
      "Test Loss: 0.0977, Test Accuracy: 23.32%\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.0957, Train Accuracy: 26.80%\n",
      "Test Loss: 0.0975, Test Accuracy: 24.70%\n",
      "Epoch [4/10]\n",
      "Train Loss: 0.0955, Train Accuracy: 27.04%\n",
      "Test Loss: 0.0974, Test Accuracy: 26.97%\n",
      "Epoch [5/10]\n",
      "Train Loss: 0.0953, Train Accuracy: 27.15%\n",
      "Test Loss: 0.0974, Test Accuracy: 22.56%\n",
      "Epoch [6/10]\n",
      "Train Loss: 0.0951, Train Accuracy: 27.15%\n",
      "Test Loss: 0.0972, Test Accuracy: 26.46%\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.0950, Train Accuracy: 27.07%\n",
      "Test Loss: 0.0972, Test Accuracy: 25.64%\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.0949, Train Accuracy: 26.61%\n",
      "Test Loss: 0.0971, Test Accuracy: 20.19%\n",
      "Epoch [9/10]\n",
      "Train Loss: 0.0948, Train Accuracy: 27.06%\n",
      "Test Loss: 0.0971, Test Accuracy: 19.50%\n",
      "Epoch [10/10]\n",
      "Train Loss: 0.0947, Train Accuracy: 26.78%\n",
      "Test Loss: 0.0970, Test Accuracy: 24.56%\n"
     ]
    }
   ],
   "source": [
    "# train and eval\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"fcnn_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss + Adam + L2 regularization\n",
    "\n",
    "```bash\n",
    "Epoch [1/10]\n",
    "Train Loss: 1.6622, Train Accuracy: 40.77%\n",
    "Test Loss: 1.5286, Test Accuracy: 46.19%\n",
    "Epoch [2/10]\n",
    "Train Loss: 1.4520, Train Accuracy: 48.63%\n",
    "Test Loss: 1.4298, Test Accuracy: 49.72%\n",
    "Epoch [3/10]\n",
    "Train Loss: 1.3521, Train Accuracy: 52.26%\n",
    "Test Loss: 1.3949, Test Accuracy: 50.66%\n",
    "Epoch [4/10]\n",
    "Train Loss: 1.2810, Train Accuracy: 54.37%\n",
    "Test Loss: 1.3669, Test Accuracy: 52.03%\n",
    "Epoch [5/10]\n",
    "Train Loss: 1.2144, Train Accuracy: 56.81%\n",
    "Test Loss: 1.3516, Test Accuracy: 53.43%\n",
    "Epoch [6/10]\n",
    "Train Loss: 1.1593, Train Accuracy: 58.65%\n",
    "Test Loss: 1.3260, Test Accuracy: 53.78%\n",
    "Epoch [7/10]\n",
    "Train Loss: 1.1110, Train Accuracy: 60.33%\n",
    "Test Loss: 1.3400, Test Accuracy: 53.27%\n",
    "Epoch [8/10]\n",
    "Train Loss: 1.0657, Train Accuracy: 61.90%\n",
    "Test Loss: 1.3611, Test Accuracy: 53.36%\n",
    "Epoch [9/10]\n",
    "...\n",
    "Test Loss: 1.3677, Test Accuracy: 53.21%\n",
    "Epoch [10/10]\n",
    "Train Loss: 0.9843, Train Accuracy: 64.45%\n",
    "Test Loss: 1.4113, Test Accuracy: 52.86%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE + Adam + L2 regularization\n",
    "\n",
    "    \n",
    "```bash\n",
    "Epoch [1/10]\n",
    "Train Loss: 0.0759, Train Accuracy: 38.59%\n",
    "Test Loss: 0.0711, Test Accuracy: 43.33%\n",
    "Epoch [2/10]\n",
    "Train Loss: 0.0717, Train Accuracy: 42.59%\n",
    "Test Loss: 0.0700, Test Accuracy: 45.26%\n",
    "Epoch [3/10]\n",
    "Train Loss: 0.0692, Train Accuracy: 45.00%\n",
    "Test Loss: 0.0687, Test Accuracy: 45.98%\n",
    "Epoch [4/10]\n",
    "Train Loss: 0.0681, Train Accuracy: 46.38%\n",
    "Test Loss: 0.0687, Test Accuracy: 46.00%\n",
    "Epoch [5/10]\n",
    "Train Loss: 0.0670, Train Accuracy: 47.24%\n",
    "Test Loss: 0.0681, Test Accuracy: 46.58%\n",
    "Epoch [6/10]\n",
    "Train Loss: 0.0664, Train Accuracy: 48.05%\n",
    "Test Loss: 0.0673, Test Accuracy: 47.33%\n",
    "Epoch [7/10]\n",
    "Train Loss: 0.0657, Train Accuracy: 48.64%\n",
    "Test Loss: 0.0671, Test Accuracy: 47.29%\n",
    "Epoch [8/10]\n",
    "Train Loss: 0.0655, Train Accuracy: 48.78%\n",
    "Test Loss: 0.0657, Test Accuracy: 48.33%\n",
    "Epoch [9/10]\n",
    "...\n",
    "Test Loss: 0.0657, Test Accuracy: 49.06%\n",
    "Epoch [10/10]\n",
    "Train Loss: 0.0647, Train Accuracy: 49.52%\n",
    "Test Loss: 0.0655, Test Accuracy: 48.77%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 loss + SDG + L2 regularization\n",
    "\n",
    "```bash\n",
    "Epoch [1/10]\n",
    "Train Loss: 0.0962, Train Accuracy: 26.48%\n",
    "Test Loss: 0.0978, Test Accuracy: 22.90%\n",
    "Epoch [2/10]\n",
    "Train Loss: 0.0959, Train Accuracy: 26.81%\n",
    "Test Loss: 0.0977, Test Accuracy: 23.32%\n",
    "Epoch [3/10]\n",
    "Train Loss: 0.0957, Train Accuracy: 26.80%\n",
    "Test Loss: 0.0975, Test Accuracy: 24.70%\n",
    "Epoch [4/10]\n",
    "Train Loss: 0.0955, Train Accuracy: 27.04%\n",
    "Test Loss: 0.0974, Test Accuracy: 26.97%\n",
    "Epoch [5/10]\n",
    "Train Loss: 0.0953, Train Accuracy: 27.15%\n",
    "Test Loss: 0.0974, Test Accuracy: 22.56%\n",
    "Epoch [6/10]\n",
    "Train Loss: 0.0951, Train Accuracy: 27.15%\n",
    "Test Loss: 0.0972, Test Accuracy: 26.46%\n",
    "Epoch [7/10]\n",
    "Train Loss: 0.0950, Train Accuracy: 27.07%\n",
    "Test Loss: 0.0972, Test Accuracy: 25.64%\n",
    "Epoch [8/10]\n",
    "Train Loss: 0.0949, Train Accuracy: 26.61%\n",
    "Test Loss: 0.0971, Test Accuracy: 20.19%\n",
    "Epoch [9/10]\n",
    "...\n",
    "Test Loss: 0.0971, Test Accuracy: 19.50%\n",
    "Epoch [10/10]\n",
    "Train Loss: 0.0947, Train Accuracy: 26.78%\n",
    "Test Loss: 0.0970, Test Accuracy: 24.56%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "# Give me a detailed conclusion of the experiments\n",
    "\n",
    "这次实验使用了三种不同的损失函数，分别是 CrossEntropyLoss，MSE 和 L1 loss。\n",
    "用了两种不同的优化器，分别是 Adam 和 SDG。还使用了 L2 正则化。\n",
    "实验结果表明， CrossEntropyLoss + Adam + L2 正则化的准确率最高，达到了52.86%。MSE + Adam + L2 正则化的准确率为49.06%，L1 loss + SDG + L2 正则化的准确率为24.56%。\n",
    "\n",
    "- 原因分析：\n",
    "\n",
    "1. CrossEntropyLoss + Adam + L2 正则化的准确率最高，可能是因为 CrossEntropyLoss 是分类问题的常用损失函数，适合多分类问题。Adam 是一种自适应学习率的优化器，能够更快地收敛。L2 正则化可以防止过拟合，提高模型的泛化能力。\n",
    "2. MSE + Adam + L2 正则化的准确率次高，可能是因为 MSE 是回归问题的常用损失函数，不太适合多分类问题。但是由于使用了 L2 正则化，可以防止过拟合，提高模型的泛化能力。\n",
    "3. L1 loss + SDG + L2 正则化的准确率最低，可能是因为 L1 loss 对异常值更敏感，容易受到异常值的影响。SDG 是一种随机梯度下降优化器，收敛速度较慢。L2 正则化可以防止过拟合，提高模型的泛化能力。\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
